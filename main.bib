
@article{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	urldate = {2021-05-11},
	journal = {arXiv:2012.12877 [cs]},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.12877},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\MN3MX9L4\\Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QH92I9AL\\2012.html:text/html},
}

@article{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	language = {en},
	urldate = {2021-01-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\EIVLVGJP\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\HJAGRCRU\\c399862d3b9d6b76c8436e924a68c45b-Abstract.html:text/html},
}

@inproceedings{rios_anime_2022,
	title = {Anime {Character} {Recognition} using {Intermediate} {Features} {Aggregation}},
	copyright = {All rights reserved},
	doi = {10.1109/ISCAS48785.2022.9937519},
	abstract = {In this work we study the problem of anime character recognition. Anime, refers to animation produced within Japan and work derived or inspired from it. We propose a novel Intermediate Features Aggregation classification head, which helps smooth the optimization landscape of Vision Transformers (ViTs) by adding skip connections between intermediate layers and the classification head, thereby improving relative classification accuracy by up to 28\%. The proposed model, named as Animesion, is the first end-to-end framework for large-scale anime character recognition. We conduct extensive experiments using a variety of classification models, including CNNs and self-attention based ViTs. We also adapt its multimodal variation Vision-Language Transformer (ViLT), to incorporate external tag data for classification, without additional multimodal pre-training. Through our results we obtain new insights into the effects of how hyperparameters such as input sequence length, mini-batch size, and variations on the architecture, affect the transfer learning performance of Vi(L)Ts.},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Rios, Edwin Arkel and Hu, Min-Chun and Lai, Bo-Cheng},
	month = may,
	year = {2022},
	note = {ISSN: 2158-1525},
	keywords = {Computer architecture, Transfer learning, Animation, Adaptation models, Circuits and systems, Sensitivity, Transformers},
	pages = {424--428},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\ZYRQAKEG\\9937519.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-08-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\BCYL2N7V\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{wei_fine-grained_2021,
	title = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}},
	doi = {10.1109/TPAMI.2021.3126648},
	abstract = {Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, which underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas – fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wei, Xiu-Shen and Song, Yi-Zhe and Mac Aodha, Oisin and Wu, Jianxin and Peng, Yuxin and Tang, Jinhui and Yang, Jian and Belongie, Serge},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, Task analysis, Image analysis, Deep Learning, Image retrieval, Visualization, Birds, Image recognition, Fine-Grained Image Recognition, Fine-Grained Image Retrieval, Fine-Grained Images Analysis},
	pages = {1--1},
	file = {Accepted Version:C\:\\Users\\edwin\\Zotero\\storage\\RTZ25B5F\\Wei et al. - 2021 - Fine-Grained Image Analysis with Deep Learning A .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\2FQD5LBI\\9609630.html:text/html},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	author = {Biewald, Lukas},
	year = {2020},
	annote = {Software available from wandb.com},
}

@inproceedings{hu_rams-trans_2021,
	address = {New York, NY, USA},
	series = {{MM} '21},
	title = {{RAMS}-{Trans}: {Recurrent} {Attention} {Multi}-scale {Transformer} for {Fine}-grained {Image} {Recognition}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {{RAMS}-{Trans}},
	doi = {10.1145/3474085.3475561},
	abstract = {In fine-grained image recognition (FGIR), the localization and amplification of region attention is an important factor, which has been explored extensively convolutional neural networks (CNNs) based approaches. The recently developed vision transformer (ViT) has achieved promising results in computer vision tasks. Compared with CNNs, Image sequentialization is a brand new manner. However, ViT is limited in its receptive field size and thus lacks local attention like CNNs due to the fixed size of its patches, and is unable to generate multi-scale features to learn discriminative region attention. To facilitate the learning of discriminative region attention without box/part annotations, we use the strength of the attention weights to measure the importance of the patch tokens corresponding to the raw images. We propose the recurrent attention multi-scale transformer (RAMS-Trans), which uses the transformer's self-attention to recursively learn discriminative region attention in a multi-scale manner. Specifically, at the core of our approach lies the dynamic patch proposal module (DPPM) responsible for guiding region amplification to complete the integration of multi-scale image patches. The DPPM starts with the full-size image patches and iteratively scales up the region attention to generate new patches from global to local by the intensity of the attention weights generated at each scale as an indicator. Our approach requires only the attention weights that come with ViT itself and can be easily trained end-to-end. Extensive experiments demonstrate that RAMS-Trans performs better than exising works, in addition to efficient CNN models, achieving state-of-the-art results on three benchmark datasets.},
	urldate = {2022-08-23},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Yunqing and Jin, Xuan and Zhang, Yin and Hong, Haiwen and Zhang, Jingfeng and He, Yuan and Xue, Hui},
	month = oct,
	year = {2021},
	keywords = {transformer, fine-grained image recognition, multi-scale, region attention},
	pages = {4239--4248},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\4GT3HZWD\\Hu et al. - 2021 - RAMS-Trans Recurrent Attention Multi-scale Transf.pdf:application/pdf},
}

@misc{hu_see_2019,
	title = {See {Better} {Before} {Looking} {Closer}: {Weakly} {Supervised} {Data} {Augmentation} {Network} for {Fine}-{Grained} {Visual} {Classification}},
	shorttitle = {See {Better} {Before} {Looking} {Closer}},
	doi = {10.48550/arXiv.1901.09891},
	abstract = {Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.},
	urldate = {2022-08-23},
	publisher = {arXiv},
	author = {Hu, Tao and Qi, Honggang and Huang, Qingming and Lu, Yan},
	month = mar,
	year = {2019},
	note = {arXiv:1901.09891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\BR6MDBEB\\Hu et al. - 2019 - See Better Before Looking Closer Weakly Supervise.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QRF9LVD6\\1901.html:text/html},
}

@inproceedings{rao_counterfactual_2021,
	title = {Counterfactual {Attention} {Learning} for {Fine}-{Grained} {Visual} {Categorization} and {Re}-{Identification}},
	language = {en},
	urldate = {2022-08-23},
	author = {Rao, Yongming and Chen, Guangyi and Lu, Jiwen and Zhou, Jie},
	year = {2021},
	pages = {1025--1034},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\U9Z7Q6IV\\Rao et al. - 2021 - Counterfactual Attention Learning for Fine-Grained.pdf:application/pdf;Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\82IWF6V8\\Rao_Counterfactual_Attention_Learning_for_Fine-Grained_Visual_Categorization_and_Re-Identificat.html:text/html},
}

@inproceedings{fu_look_2017,
	title = {Look {Closer} to {See} {Better}: {Recurrent} {Attention} {Convolutional} {Neural} {Network} for {Fine}-{Grained} {Image} {Recognition}},
	shorttitle = {Look {Closer} to {See} {Better}},
	doi = {10.1109/CVPR.2017.476},
	abstract = {Recognizing fine-grained categories (e.g., bird species) is difficult due to the challenges of discriminative region localization and fine-grained feature learning. Existing approaches predominantly solve these challenges independently, while neglecting the fact that region detection and fine-grained feature learning are mutually correlated and thus can reinforce each other. In this paper, we propose a novel recurrent attention convolutional neural network (RA-CNN) which recursively learns discriminative region attention and region-based feature representation at multiple scales in a mutual reinforced way. The learning at each scale consists of a classification sub-network and an attention proposal sub-network (APN). The APN starts from full images, and iteratively generates region attention from coarse to fine by taking previous prediction as a reference, while the finer scale network takes as input an amplified attended region from previous scale in a recurrent way. The proposed RA-CNN is optimized by an intra-scale classification loss and an inter-scale ranking loss, to mutually learn accurate region attention and fine-grained representation. RA-CNN does not need bounding box/part annotations and can be trained end-to-end. We conduct comprehensive experiments and show that RA-CNN achieves the best performance in three fine-grained tasks, with relative accuracy gains of 3.3\%, 3.7\%, 3.8\%, on CUB Birds, Stanford Dogs and Stanford Cars, respectively.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Fu, Jianlong and Zheng, Heliang and Mei, Tao},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Neural networks, Feature extraction, Proposals, Visualization, Birds, Image recognition},
	pages = {4476--4484},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\LT2G5SML\\8099959.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\B2RKVYQ7\\Fu et al. - 2017 - Look Closer to See Better Recurrent Attention Con.pdf:application/pdf},
}

@inproceedings{yang_learning_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning to {Navigate} for {Fine}-{Grained} {Classification}},
	isbn = {978-3-030-01264-9},
	doi = {10.1007/978-3-030-01264-9_26},
	abstract = {Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box/part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Yang, Ze and Luo, Tiange and Wang, Dong and Hu, Zhiqiang and Gao, Jun and Wang, Liwei},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Fine-grained Classification, Ground Truth Class, Informative Regions, Scrutinizing, Stanford Cars},
	pages = {438--454},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\XKM3H99F\\Yang et al. - 2018 - Learning to Navigate for Fine-Grained Classificati.pdf:application/pdf},
}

@inproceedings{gwilliam_fair_2021,
	title = {Fair {Comparison}: {Quantifying} {Variance} in {Results} for {Fine}-{Grained} {Visual} {Categorization}},
	shorttitle = {Fair {Comparison}},
	language = {en},
	urldate = {2022-08-23},
	author = {Gwilliam, Matthew and Teuscher, Adam and Anderson, Connor and Farrell, Ryan},
	year = {2021},
	pages = {3309--3318},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\DYYAHUU8\\Gwilliam et al. - 2021 - Fair Comparison Quantifying Variance in Results f.pdf:application/pdf;Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\3V8F5A66\\Gwilliam_Fair_Comparison_Quantifying_Variance_in_Results_for_Fine-Grained_Visual_Categorization.html:text/html},
}

@inproceedings{zhang_part-based_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Part-{Based} {R}-{CNNs} for {Fine}-{Grained} {Category} {Detection}},
	isbn = {978-3-319-10590-1},
	doi = {10.1007/978-3-319-10590-1_54},
	abstract = {Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhang, Ning and Donahue, Jeff and Girshick, Ross and Darrell, Trevor},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {object detection, convolutional models, Fine-grained recognition},
	pages = {834--849},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\XYUGFKUC\\Zhang et al. - 2014 - Part-Based R-CNNs for Fine-Grained Category Detect.pdf:application/pdf},
}

@article{wei_mask-cnn_2018,
	title = {Mask-{CNN}: {Localizing} parts and selecting descriptors for fine-grained bird species categorization},
	volume = {76},
	issn = {0031-3203},
	shorttitle = {Mask-{CNN}},
	doi = {10.1016/j.patcog.2017.10.002},
	abstract = {Fine-grained image recognition is a challenging computer vision problem, due to the small inter-class variations caused by highly similar subordinate categories, and the large intra-class variations in poses, scales and rotations. In this paper, we prove that selecting useful deep descriptors contributes well to fine-grained image recognition. Specifically, a novel Mask-CNN model without the fully connected layers is proposed. Based on the part annotations, the proposed model consists of a fully convolutional network to both locate the discriminative parts (e.g., head and torso), and more importantly generate weighted object/part masks for selecting useful and meaningful convolutional descriptors. After that, a three-stream Mask-CNN model is built for aggregating the selected object- and part-level descriptors simultaneously. Thanks to discarding the parameter redundant fully connected layers, our Mask-CNN has a small feature dimensionality and efficient inference speed by comparing with other fine-grained approaches. Furthermore, we obtain a new state-of-the-art accuracy on two challenging fine-grained bird species categorization datasets, which validates the effectiveness of both the descriptor selection scheme and the proposed Mask-CNN model.},
	language = {en},
	urldate = {2022-08-23},
	journal = {Pattern Recognition},
	author = {Wei, Xiu-Shen and Xie, Chen-Wei and Wu, Jianxin and Shen, Chunhua},
	month = apr,
	year = {2018},
	keywords = {Fine-grained image recognition, Deep descriptor selection, Part localization},
	pages = {704--714},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\UWB69H68\\S0031320317303990.html:text/html},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut {Learning} in {Deep} {Neural} {Networks}},
	volume = {2},
	issn = {2522-5839},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	number = {11},
	urldate = {2022-08-19},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	note = {arXiv:2004.07780 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition, Computational science, Human behaviour, Information technology, Network models},
	pages = {665--673},
	annote = {Comment: perspective article published at Nature Machine Intelligence (https://doi.org/10.1038/s42256-020-00257-z)},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2020-12-14},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\7XLVFF8L\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\WMXE926K\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\VQ2D9SJR\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\TEA5C6AT\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\4TBHJTHS\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\L46UW9JK\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\5H3FWVBB\\2010.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\C8VA2DFU\\2010.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-01-12},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\S8G9YEBZ\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\2V8SGB3J\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\AKNTY5T5\\1810.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\FY49HZR2\\1810.html:text/html},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-11-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\ERIG3UGW\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\TLIL75XD\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\GQXASNS8\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\D9GJSWY8\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\JJX2GDIP\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\DUY63VKJ\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QUUN6JQZ\\1512.html:text/html;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\NUHLZPKZ\\1512.html:text/html},
}

@inproceedings{wang_feature_2021,
	title = {Feature {Fusion} {Vision} {Transformer} for {Fine}-{Grained} {Visual} {Categorization}},
	abstract = {The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.},
	urldate = {2021-12-29},
	booktitle = {British {Machine} {Vision} {Conference} ({BMVC})},
	author = {Wang, Jun and Yu, Xiaohan and Gao, Yongsheng},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02341},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages, 2 figures, 3 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\6WL43BUX\\Wang et al. - 2021 - Feature Fusion Vision Transformer for Fine-Grained.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\45PXFJLX\\2107.html:text/html},
}

@article{wah_caltech-ucsd_nodate,
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
	abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and attribute labels. Images and annotations were ﬁltered by multiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
	language = {en},
	author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	pages = {8},
	file = {Wah et al. - The Caltech-UCSD Birds-200-2011 Dataset.pdf:C\:\\Users\\edwin\\Zotero\\storage\\IJ4U82PD\\Wah et al. - The Caltech-UCSD Birds-200-2011 Dataset.pdf:application/pdf},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\8LP22X5W\\Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\U2QCRQWS\\1607.html:text/html},
}

@inproceedings{he_transfg_2022,
	title = {{TransFG}: {A} {Transformer} {Architecture} for {Fine}-{Grained} {Recognition}},
	shorttitle = {{TransFG}},
	abstract = {Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.},
	language = {en},
	urldate = {2023-02-26},
	booktitle = {Proceedings of the {First} {MiniCon} {Conference}},
	author = {He, Ju and Chen, Jie-Neng and Liu, Shuai and Kortylewski, Adam and Yang, Cheng and Bai, Yutong and Wang, Changhu},
	month = feb,
	year = {2022},
}

@misc{abnar_quantifying_2020,
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	doi = {10.48550/arXiv.2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2023-02-26},
	publisher = {arXiv},
	author = {Abnar, Samira and Zuidema, Willem},
	month = may,
	year = {2020},
	note = {arXiv:2005.00928 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\75B7RA2R\\Abnar and Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\NQXMCWIK\\2005.html:text/html},
}

@article{yu_patchy_2020,
	title = {Patchy {Image} {Structure} {Classification} {Using} {Multi}-{Orientation} {Region} {Transform}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	doi = {10.1609/aaai.v34i07.6968},
	abstract = {Exterior contour and interior structure are both vital features for classifying objects. However, most of the existing methods consider exterior contour feature and internal structure feature separately, and thus fail to function when classifying patchy image structures that have similar contours and flexible structures. To address above limitations, this paper proposes a novel Multi-Orientation Region Transform (MORT), which can effectively characterize both contour and structure features simultaneously, for patchy image structure classification. MORT is performed over multiple orientation regions at multiple scales to effectively integrate patchy features, and thus enables a better description of the shape in a coarse-to-fine manner. Moreover, the proposed MORT can be extended to combine with the deep convolutional neural network techniques, for further enhancement of classification accuracy. Very encouraging experimental results on the challenging ultra-fine-grained cultivar recognition task, insect wing recognition task, and large variation butterfly recognition task are obtained, which demonstrate the effectiveness and superiority of the proposed MORT over the state-of-the-art methods in classifying patchy image structures. Our code and three patchy image structure datasets are available at: https://github.com/XiaohanYu-GU/MReT2019.},
	language = {en},
	number = {07},
	urldate = {2023-03-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yu, Xiaohan and Zhao, Yang and Gao, Yongsheng and Xiong, Shengwu and Yuan, Xiaohui},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {12741--12748},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\R5RNF6N9\\Yu et al. - 2020 - Patchy Image Structure Classification Using Multi-.pdf:application/pdf},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	abstract = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to \$26\$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within \$0.8\%\$ of the performance of full fine-tuning, adding only \$3.6\%\$ parameters per task. By contrast, fine-tuning trains \$100\%\$ of the parameters per task.},
	language = {en},
	urldate = {2023-04-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2790--2799},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\P8BUUF6L\\Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf;Supplementary PDF:C\:\\Users\\edwin\\Zotero\\storage\\BFAAVS4E\\Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf},
}

@inproceedings{lin_bilinear_2015,
	address = {Santiago, Chile},
	title = {Bilinear {CNN} {Models} for {Fine}-{Grained} {Visual} {Recognition}},
	isbn = {978-1-4673-8391-2},
	doi = {10.1109/ICCV.2015.170},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yu and RoyChowdhury, Aruni and Maji, Subhransu},
	month = dec,
	year = {2015},
	pages = {1449--1457},
	file = {Lin et al. - 2015 - Bilinear CNN Models for Fine-Grained Visual Recogn.pdf:C\:\\Users\\edwin\\Zotero\\storage\\8LNKE86E\\Lin et al. - 2015 - Bilinear CNN Models for Fine-Grained Visual Recogn.pdf:application/pdf},
}

@misc{jie_convolutional_2022,
	title = {Convolutional {Bypasses} {Are} {Better} {Vision} {Transformer} {Adapters}},
	doi = {10.48550/arXiv.2207.07039},
	abstract = {The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5\% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Jie, Shibo and Deng, Zhi-Hong},
	month = aug,
	year = {2022},
	note = {arXiv:2207.07039 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\GJSFMPB4\\Jie and Deng - 2022 - Convolutional Bypasses Are Better Vision Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QDF8QVPH\\2207.html:text/html},
}

@inproceedings{sun_sim-trans_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {{SIM}-{Trans}: {Structure} {Information} {Modeling} {Transformer} for {Fine}-grained {Visual} {Categorization}},
	isbn = {978-1-4503-9203-7},
	shorttitle = {{SIM}-{Trans}},
	doi = {10.1145/3503161.3548308},
	abstract = {Fine-grained visual categorization (FGVC) aims at recognizing objects from similar subordinate categories, which is challenging and practical for human's accurate automatic recognition needs. Most FGVC approaches focus on the attention mechanism research for discriminative regions mining while neglecting their interdependencies and composed holistic object structure, which are essential for model's discriminative information localization and understanding ability. To address the above limitations, we propose the Structure Information Modeling Transformer (SIM-Trans) to incorporate object structure information into transformer for enhancing discriminative representation learning to contain both the appearance information and structure information. Specifically, we encode the image into a sequence of patch tokens and build a strong vision transformer framework with two well-designed modules: (i) the structure information learning (SIL) module is proposed to mine the spatial context relation of significant patches within the object extent with the help of the transformer's self-attention weights, which is further injected into the model for importing structure information; (ii) the multi-level feature boosting (MFB) module is introduced to exploit the complementary of multi-level features and contrastive learning among classes to enhance feature robustness for accurate fine-grained visual categorization. The proposed two modules are light-weighted and can be plugged into any transformer network and trained end-to-end easily, which only depends on the attention weights that come with the vision transformer itself. Extensive experiments and analyses demonstrate that the proposed SIM-Trans achieves state-of-the-art performance on fine-grained visual categorization benchmarks. The code will be available at https://github.com/PKU-ICST-MIPL/SIM-Trans\_ACMMM2022.},
	urldate = {2024-07-14},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Hongbo and He, Xiangteng and Peng, Yuxin},
	month = oct,
	year = {2022},
	pages = {5853--5861},
	file = {Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\598RHGU5\\Sun et al. - 2022 - SIM-Trans Structure Information Modeling Transfor.pdf:application/pdf},
}

@article{ye_image_2024,
	title = {The {Image} {Data} and {Backbone} in {Weakly} {Supervised} {Fine}-{Grained} {Visual} {Categorization}: {A} {Revisit} and {Further} {Thinking}},
	volume = {34},
	issn = {1558-2205},
	shorttitle = {The {Image} {Data} and {Backbone} in {Weakly} {Supervised} {Fine}-{Grained} {Visual} {Categorization}},
	doi = {10.1109/TCSVT.2023.3284405},
	abstract = {Weakly-supervised fine-grained visual categorization (FGVC) aims to achieve subclass classification within the same large class using only label information. Compared to general images, fine-grained images have similar appearances and features, and are often affected by disturbances such as viewpoint, lighting, and occlusion during data collection, resulting in significant intra-class variance and small inter-class variance. To achieve FGVC, carefully designed models are often needed to explore the locally discriminative regions of the image. This paper revisits high-quality FGVC publications based on deep learning and analyzes from two new perspective: fine-grained image data and backbone. We address two ignored but interesting problems in FGVC. First, we argue that the reasons for exacerbating intra-class variance are not the same in data of animal, plant, and commodity types, and it is necessary to consider the effects of posture, covariate shift, and structural changes. Additionally, the “soft boundary” between subclasses intensifies the difficulty of classification. Second, we highlight that convolutional networks and self-attention networks have different receptive fields and shape biases, leading to performance differences when processing different types of fine-grained data. Overall, our analysis provides new insights into recent advances, challenges, and future directions for FGVC based on deep learning, which can help researchers develop more effective models for FGVC.},
	number = {1},
	urldate = {2024-07-15},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Ye, Shuo and Wang, Yu and Peng, Qinmu and You, Xinge and Chen, C. L. Philip},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Data models, deep learning, Training, Visualization, Birds, Annotations, Automobiles, Fine-grained visual categorization, Analytical models, weakly supervised learning},
	pages = {2--16},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\H6GAZHZT\\10147290.html:text/html},
}

@misc{rios_global-local_2024,
	title = {Global-{Local} {Similarity} for {Efficient} {Fine}-{Grained} {Image} {Recognition} with {Vision} {Transformers}},
	copyright = {All rights reserved},
	abstract = {Fine-grained recognition involves the classification of images from subordinate macro-categories, and it is challenging due to small inter-class differences. To overcome this, most methods perform discriminative feature selection enabled by a feature extraction backbone followed by a high-level feature refinement step. Recently, many studies have shown the potential behind vision transformers as a backbone for fine-grained recognition, but their usage of its attention mechanism to select discriminative tokens can be computationally expensive. In this work, we propose a novel and computationally inexpensive metric to identify discriminative regions in an image. We compare the similarity between the global representation of an image given by the CLS token, a learnable token used by transformers for classification, and the local representation of individual patches. We select the regions with the highest similarity to obtain crops, which are forwarded through the same transformer encoder. Finally, high-level features of the original and cropped representations are further refined together in order to make more robust predictions. Through extensive experimental evaluation we demonstrate the effectiveness of our proposed method, obtaining favorable results in terms of accuracy across a variety of datasets. Furthermore, our method achieves these results at a much lower computational cost compared to the alternatives. Code and checkpoints are available at: {\textbackslash}url\{https://github.com/arkel23/GLSim\}.},
	urldate = {2024-07-19},
	publisher = {arXiv},
	author = {Rios, Edwin Arkel and Hu, Min-Chun and Lai, Bo-Cheng},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2, I.4},
	annote = {Comment: Main: 12 pages, 5 figures, 5 tables. Appendix: 9 pages, 9 figures, 10 tables. Total: 21 pages, 14 figures, 15 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\MH2L9C7S\\Rios et al. - 2024 - Global-Local Similarity for Efficient Fine-Grained.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\IYYK6NSA\\2407.html:text/html},
}

@inproceedings{he_masked_2022,
	address = {New Orleans, LA, USA},
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	doi = {10.1109/CVPR52688.2022.01553},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
	month = jun,
	year = {2022},
	pages = {15979--15988},
	file = {He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf:C\:\\Users\\edwin\\Zotero\\storage\\ZXDFJZ59\\He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	doi = {10.1109/ICCV48922.2021.00951},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2024-07-25},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Computer architecture, Training, Computer vision, Image segmentation, Semantics, Image retrieval, Layout, Recognition and classification, Representation learning, Transfer/Low-shot/Semi/Unsupervised Learning},
	pages = {9630--9640},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\89N55RQF\\9709990.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\CPRNWBW4\\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf:application/pdf},
}

@inproceedings{evci_head2toe_2022,
	title = {{Head2Toe}: {Utilizing} {Intermediate} {Representations} for {Better} {Transfer} {Learning}},
	shorttitle = {{Head2Toe}},
	abstract = {Transfer-learning methods aim to improve performance in a data-scarce target domain using a model pretrained on a data-rich source domain. A cost-efficient strategy, linear probing, involves freezing the source model and training a new classification head for the target domain. This strategy is outperformed by a more costly but state-of-the-art method – fine-tuning all parameters of the source model to the target domain – possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later previously trained layers. We explore the hypothesis that these intermediate layers might be directly exploited. We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the Visual Task Adaptation Benchmark-1k, Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning. Code used in our experiments can be found in supplementary materials.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Evci, Utku and Dumoulin, Vincent and Larochelle, Hugo and Mozer, Michael C.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {6009--6033},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\HS77R6MR\\Evci et al. - 2022 - Head2Toe Utilizing Intermediate Representations f.pdf:application/pdf},
}

@inproceedings{yu_benchmark_2021,
	title = {Benchmark {Platform} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization} {Beyond} {Human} {Performance}},
	doi = {10.1109/ICCV48922.2021.01012},
	abstract = {Deep learning methods have achieved remarkable success in fine-grained visual categorization. Such successful categorization at sub-ordinate level, e.g., different animal or plant species, however relies heavily on the visual differences that human can observe and the ground-truths are labelled on the basis of such human visual observation. In contrast, few research has been done for visual categorization at the ultra-fine-grained level, i.e., a granularity where even human experts can hardly identify the visual differences or are not yet able to give affirmative labels by inferring observed pattern differences. This paper reports our efforts towards mitigating this research gap. We introduce the ultra-fine-grained (UFG) image dataset, a large collection of 47,114 images from 3,526 categories. All the images in the proposed UFG image dataset are grouped into categories with different confirmed cultivar names. In addition, we perform an extensive evaluation of state-of-the-art fine-grained classification methods on the proposed UFG image dataset as comparative baselines. The proposed UFG image dataset and evaluation protocols is intended to serve as a benchmark platform that can advance research of visual classification from approaching human performance to beyond human ability, via facilitating benchmark data of artificial intelligence (AI) not to be limited by the labels of human intelligence (HI). The dataset is available online at https://githuh.com/XiaohanYu-GU/Ultra-FGVC.},
	urldate = {2024-07-25},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yu, Xiaohan and Zhao, Yang and Gao, Yongsheng and Yuan, Xiaohui and Xiong, Shengwu},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Deep learning, Benchmark testing, Training, Computer vision, Protocols, Visualization, Annotations, Recognition and classification, and cell microscopy, biological, Datasets and evaluation, Medical},
	pages = {10265--10275},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\8AEJYC2I\\9710088.html:text/html},
}

@misc{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	doi = {10.48550/arXiv.1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv:1704.04861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\5ARMECWG\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\QDLVDVVI\\1704.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2024-07-25},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@article{yu_mix-vit_2023,
	title = {Mix-{ViT}: {Mixing} attentive vision transformer for ultra-fine-grained visual categorization},
	volume = {135},
	issn = {0031-3203},
	shorttitle = {Mix-{ViT}},
	doi = {10.1016/j.patcog.2022.109131},
	abstract = {Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e., classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT},
	urldate = {2024-07-25},
	journal = {Pattern Recognition},
	author = {Yu, Xiaohan and Wang, Jun and Zhao, Yang and Gao, Yongsheng},
	month = mar,
	year = {2023},
	keywords = {Vision transformer, Attentive mixing, Self-supervised learning, Ultra-fine-grained visual categorization},
	pages = {109131},
}

@article{fang_learning_2024,
	title = {Learning {Contrastive} {Self}-{Distillation} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization} {Targeting} {Limited} {Samples}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2024.3370731},
	abstract = {In the field of intelligent multimedia analysis, ultra-fine-grained visual categorization (Ultra-FGVC) plays a vital role in distinguishing intricate subcategories within broader categories. However, this task is inherently challenging due to the complex granularity of category subdivisions and the limited availability of data for each category. To address these challenges, this work proposes CSDNet, a pioneering framework that effectively explores contrastive learning and self-distillation to learn discriminative representations specifically designed for Ultra-FGVC tasks. CSDNet comprises three main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic Discrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer (SSDT), which collectively enhance the generalization of deep models across instance, feature, and logit prediction levels. To increase the diversity of training samples, the SSDP module introduces adaptive augmented samples to spotlight subcategory-specific discrepancies. Simultaneously, the proposed DDL module stores historical intermediate features by a dynamic memory queue, which optimizes the feature learning space through iterative contrastive learning. Furthermore, the SSDT module effectively distills subcategory-specific discrepancies knowledge from the inherent structure of limited training data using a self-distillation paradigm at the logit prediction level. Experimental results demonstrate that CSDNet outperforms current state-of-the-art Ultra-FGVC methods, emphasizing its powerful efficacy and adaptability in addressing Ultra-FGVC tasks.},
	urldate = {2024-07-25},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Fang, Ziye and Jiang, Xin and Tang, Hao and Li, Zechao},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Task analysis, Training, Feature extraction, Visualization, Circuits and systems, Self-supervised learning, Ultra-fine-grained visual categorization, contrastive learning, Data augmentation, self-distillation},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\2JK7E8XA\\10445701.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\IJSINRG3\\Fang et al. - 2024 - Learning Contrastive Self-Distillation for Ultra-F.pdf:application/pdf},
}

@inproceedings{yu_cle-vit_2023,
	address = {Macau, SAR China},
	title = {{CLE}-{ViT}: {Contrastive} {Learning} {Encoded} {Transformer} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization}},
	isbn = {978-1-956792-03-4},
	shorttitle = {{CLE}-{ViT}},
	doi = {10.24963/ijcai.2023/504},
	abstract = {Ultra-fine-grained visual classification (ultra-FGVC) targets at classifying sub-grained categories of fine-grained objects. This inevitably requires discriminative representation learning within a limited training set. Exploring intrinsic features from the object itself, e.g., predicting the rotation of a given image, has demonstrated great progress towards learning discriminative representation. Yet none of these works consider explicit supervision for learning mutual information at instance level. To this end, this paper introduces CLE-ViT, a novel contrastive learning encoded transformer, to address the fundamental problem in ultra-FGVC. The core design is a self-supervised module that performs self-shuffling and masking and then distinguishes these altered images from other images. This drives the model to learn an optimized feature space that has a large inter-class distance while remaining tolerant to intra-class variations. By incorporating this self-supervised module, the network acquires more knowledge from the intrinsic structure of the input data, which improves the generalization ability without requiring extra manual annotations. CLE-ViT demonstrates strong performance on 7 publicly available datasets, demonstrating its effectiveness in the ultra-FGVC task. The code is available at https://github.com/Markin-Wang/CLEViT.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Yu, Xiaohan and Wang, Jun and Gao, Yongsheng},
	month = aug,
	year = {2023},
	pages = {4531--4539},
	file = {Yu et al. - 2023 - CLE-ViT Contrastive Learning Encoded Transformer .pdf:C\:\\Users\\edwin\\Zotero\\storage\\6UN3KBA3\\Yu et al. - 2023 - CLE-ViT Contrastive Learning Encoded Transformer .pdf:application/pdf},
}

@article{liu_novel_nodate,
	title = {Novel {Class} {Discovery} for {Ultra}-{Fine}-{Grained} {Visual} {Categorization}},
	abstract = {Ultra-fine-grained visual categorization (Ultra-FGVC) aims at distinguishing highly similar sub-categories within fine-grained objects, such as different soybean cultivars. Compared to traditional fine-grained visual categorization, Ultra-FGVC encounters more hurdles due to the small inter-class and large intra-class variation. Given these challenges, relying on human annotation for UltraFGVC is impractical. To this end, our work introduces a novel task termed Ultra-Fine-Grained Novel Class Discovery (UFG-NCD), which leverages partially annotated data to identify new categories of unlabeled images for Ultra-FGVC. To tackle this problem, we devise a RegionAligned Proxy Learning (RAPL) framework, which comprises a Channel-wise Region Alignment (CRA) module and a Semi-Supervised Proxy Learning (SemiPL) strategy. The CRA module is designed to extract and utilize discriminative features from local regions, facilitating knowledge transfer from labeled to unlabeled classes. Furthermore, SemiPL strengthens representation learning and knowledge transfer with proxy-guided supervised learning and proxyguided contrastive learning. Such techniques leverage class distribution information in the embedding space, improving the mining of subtle differences between labeled and unlabeled ultra-fine-grained classes. Extensive experiments demonstrate that RAPL significantly outperforms baselines across various datasets, indicating its effectiveness in handling the challenges of UFG-NCD. Code is available at https://github.com/SSDUT-Caiyq/UFG-NCD.},
	language = {en},
	author = {Liu, Yu and Cai, Yaqi and Jia, Qi and Qiu, Binglin and Wang, Weimin and Pu, Nan},
	file = {Liu et al. - Novel Class Discovery for Ultra-Fine-Grained Visua.pdf:C\:\\Users\\edwin\\Zotero\\storage\\FKAP4E7Y\\Liu et al. - Novel Class Discovery for Ultra-Fine-Grained Visua.pdf:application/pdf},
}

@article{yu_maskcov_2021,
	title = {{MaskCOV}: {A} random mask covariance network for ultra-fine-grained visual categorization},
	volume = {119},
	issn = {0031-3203},
	shorttitle = {{MaskCOV}},
	doi = {10.1016/j.patcog.2021.108067},
	abstract = {Ultra-fine-grained visual categorization (ultra-FGVC) categorizes objects with more similar patterns between classes than those in fine-grained visual categorization (FGVC), e.g., where the spectrum of granularity significantly moves down from classifying species to classifying cultivars within the same species. It is considered as an open research problem mainly due to the following challenges. First, the inter-class differences among images are much smaller by level of orders (e.g., cultivars in the same species) than those in current FGVC tasks (e.g., species). Second, there is only a few samples per category, which is beyond the ability of most large training data favored convolutional neural network methods. To address these problems, we propose a novel random mask covariance network (MaskCOV), which integrates an auxiliary self-supervised learning module with a powerful in-image data augmentation scheme for the ultra-FGVC. Specifically, we first uniformly partition input images into patches and then augment data by randomly shuffling and masking these patches. On top of that, we introduce an auxiliary self-supervised learning module of predicting the spatial covariance context of these patches to increase discriminability of our network for classification. Very encouraging experimental results of the proposed method in comparison with the state-of-the-art benchmarks demonstrate its superiority and potential of MaskCOV concept, which pushes research boundary forward from the fine-grained to the ultra-fine-grained visual categorization.},
	urldate = {2024-08-02},
	journal = {Pattern Recognition},
	author = {Yu, Xiaohan and Zhao, Yang and Gao, Yongsheng and Xiong, Shengwu},
	month = nov,
	year = {2021},
	keywords = {Fine-grained visual categorization, Self-supervised learning, Ultra-fine-grained visual categorization, Covariance matrix},
	pages = {108067},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\6KLVKHBL\\S0031320321002545.html:text/html},
}

@inproceedings{shi_layerconnect_2022,
	address = {Gyeongju, Republic of Korea},
	title = {{LayerConnect}: {Hypernetwork}-{Assisted} {Inter}-{Layer} {Connector} to {Enhance} {Parameter} {Efficiency}},
	shorttitle = {{LayerConnect}},
	abstract = {Pre-trained Language Models (PLMs) are the cornerstone of the modern Natural Language Processing (NLP). However, as PLMs become heavier, fine tuning all their parameters loses their efficiency. Existing parameter-efficient methods generally focus on reducing the trainable parameters in PLMs but neglect the inference speed, which limits the ability to deploy PLMs. In this paper, we propose LayerConnect (hypernetwork-assisted inter-layer connectors) to enhance inference efficiency. Specifically, a light-weight connector with a linear structure is inserted between two Transformer layers, and the parameters inside each connector are tuned by a hypernetwork comprising an interpolator and a down-sampler. We perform extensive experiments on the widely used the GLUE benchmark. The experimental results verify the inference efficiency of our model. Compared to Adapter, our model parameters are reduced to approximately 11.75\%, while the performance degradation is kept to less than 5\% (2.5 points on average).},
	urldate = {2024-08-02},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Shi, Haoxiang and Zhang, Rongsheng and Wang, Jiaan and Wang, Cen and Zheng, Yinhe and Sakai, Tetsuya},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {3120--3126},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\TVIZH3TV\\Shi et al. - 2022 - LayerConnect Hypernetwork-Assisted Inter-Layer Co.pdf:application/pdf},
}

@incollection{avidan_visual_2022,
	address = {Cham},
	title = {Visual {Prompt} {Tuning}},
	volume = {13693},
	isbn = {978-3-031-19826-7 978-3-031-19827-4},
	abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19827-4_41},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {709--727},
	file = {Jia et al. - 2022 - Visual Prompt Tuning.pdf:C\:\\Users\\edwin\\Zotero\\storage\\GV728NGK\\Jia et al. - 2022 - Visual Prompt Tuning.pdf:application/pdf},
}

@inproceedings{tu_visual_2023,
	address = {Vancouver, BC, Canada},
	title = {Visual {Query} {Tuning}: {Towards} {Effective} {Usage} of {Intermediate} {Representations} for {Parameter} and {Memory} {Efficient} {Transfer} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {Visual {Query} {Tuning}},
	doi = {10.1109/CVPR52729.2023.00746},
	abstract = {Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is kept frozen. The key challenge is how to utilize these intermediate features given their gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable “query” tokens to each layer, VQT leverages the inner workings of Transformers to “summarize” rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efﬁciency in training, compared to many other parameterefﬁcient ﬁne-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in transfer learning. Empirically, VQT consistently surpasses the state-of-the-art approach that utilizes intermediate features for transfer learning and outperforms full ﬁne-tuning in many cases. Compared to parameter-efﬁcient approaches that adapt features, VQT achieves much higher accuracy under memory constraints. Most importantly, VQT is compatible with these approaches to attain even higher accuracy, making it a simple addon to further boost transfer learning. Code is available at https://github.com/andytu28/VQT.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tu, Cheng-Hao and Mai, Zheda and Chao, Wei-Lun},
	month = jun,
	year = {2023},
	pages = {7725--7735},
	file = {Tu et al. - 2023 - Visual Query Tuning Towards Effective Usage of In.pdf:C\:\\Users\\edwin\\Zotero\\storage\\UF57A9ES\\Tu et al. - 2023 - Visual Query Tuning Towards Effective Usage of In.pdf:application/pdf},
}

@misc{zhou_deepvit_2021,
	title = {{DeepViT}: {Towards} {Deeper} {Vision} {Transformer}},
	shorttitle = {{DeepViT}},
	doi = {10.48550/arXiv.2103.11886},
	abstract = {Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6\% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit\_repo.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Zhou, Daquan and Kang, Bingyi and Jin, Xiaojie and Yang, Linjie and Lian, Xiaochen and Jiang, Zihang and Hou, Qibin and Feng, Jiashi},
	month = apr,
	year = {2021},
	note = {arXiv:2103.11886 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\Z3R5NFRP\\Zhou et al. - 2021 - DeepViT Towards Deeper Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\V8DB25GV\\2103.html:text/html},
}

@misc{gong_vision_2021,
	title = {Vision {Transformers} with {Patch} {Diversification}},
	doi = {10.48550/arXiv.2104.12753},
	abstract = {Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Gong, Chengyue and Wang, Dilin and Li, Meng and Chandra, Vikas and Liu, Qiang},
	month = jun,
	year = {2021},
	note = {arXiv:2104.12753 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: preprint},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\FN9NDR4K\\Gong et al. - 2021 - Vision Transformers with Patch Diversification.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\FFDNXU3J\\2104.html:text/html},
}

@incollection{avidan_improving_2022,
	address = {Cham},
	title = {Improving {Vision} {Transformers} by {Revisiting} {High}-{Frequency} {Components}},
	volume = {13684},
	isbn = {978-3-031-20052-6 978-3-031-20053-3},
	abstract = {The transformer models have shown promising effectiveness in dealing with various vision tasks. However, compared with training Convolutional Neural Network (CNN) models, training Vision Transformer (ViT) models is more difficult and relies on the large-scale training set. To explain this observation we make a hypothesis that ViT models are less effective in capturing the high-frequency components of images than CNN models, and verify it by a frequency analysis. Inspired by this finding, we first investigate the effects of existing techniques for improving ViT models from a new frequency perspective, and find that the success of some techniques (e.g., RandAugment) can be attributed to the better usage of the high-frequency components. Then, to compensate for this insufficient ability of ViT models, we propose HAT, which directly augments high-frequency components of images via adversarial training. We show that HAT can consistently boost the performance of various ViT models (e.g., +1.2\% for ViT-B, +0.5\% for Swin-B), and especially enhance the advanced model VOLO-D5 to 87.3\% that only uses ImageNet-1K data, and the superiority can also be maintained on out-of-distribution data and transferred to downstream tasks. The code is available at: https://github.com/jiawangbai/HAT.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Bai, Jiawang and Yuan, Li and Xia, Shu-Tao and Yan, Shuicheng and Li, Zhifeng and Liu, Wei},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-20053-3_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--18},
	file = {Bai et al. - 2022 - Improving Vision Transformers by Revisiting High-F.pdf:C\:\\Users\\edwin\\Zotero\\storage\\L4D4K8ZM\\Bai et al. - 2022 - Improving Vision Transformers by Revisiting High-F.pdf:application/pdf},
}

@inproceedings{chen_principle_2022,
	address = {New Orleans, LA, USA},
	title = {The {Principle} of {Diversity}: {Training} {Stronger} {Vision} {Transformers} {Calls} for {Reducing} {All} {Levels} of {Redundancy}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	shorttitle = {The {Principle} of {Diversity}},
	doi = {10.1109/CVPR52688.2022.01171},
	abstract = {Vision transformers (ViTs) have gained increasing popularity as they are commonly believed to own higher modeling capacity and representation flexibility, than traditional convolutional networks. However, it is questionable whether such potential has been fully unleashed in practice, as the learned ViTs often suffer from over-smoothening, yielding likely redundant models. Recent works made preliminary attempts to identify and alleviate such redundancy, e.g., via regularizing embedding similarity or re-injecting convolution-like structures. However, a “head-to-toe assessment” regarding the extent of redundancy in ViTs, and how much we could gain by thoroughly mitigating such, has been absent for this field. This paper, for the first time, systematically studies the ubiquitous existence of redundancy at all three levels: patch embedding, attention map, and weight space. In view of them, we advocate a principle of diversity for training ViTs, by presenting corresponding regularizers that encourage the representation diversity and coverage at each of those levels, that enabling capturing more discriminative information. Extensive experiments on ImageNet with a number of ViT backbones validate the effectiveness of our proposals, largely eliminating the observed ViT redundancy and significantly boosting the model generalization. For example, our diversified DeiT obtains 0.70\% ∼ 1.76\% accuracy boosts on ImageNet with highly reduced similarity. Our codes are fully available in https://github.com/VITA-Group/Diverse-ViT.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Tianlong and Zhang, Zhenyu and Cheng, Yu and Awadallah, Ahmed and Wang, Zhangyang},
	month = jun,
	year = {2022},
	pages = {12010--12020},
	file = {Chen et al. - 2022 - The Principle of Diversity Training Stronger Visi.pdf:C\:\\Users\\edwin\\Zotero\\storage\\3FZDPDM5\\Chen et al. - 2022 - The Principle of Diversity Training Stronger Visi.pdf:application/pdf},
}

@misc{wang_anti-oversmoothing_2022,
	title = {Anti-{Oversmoothing} in {Deep} {Vision} {Transformers} via the {Fourier} {Domain} {Analysis}: {From} {Theory} to {Practice}},
	shorttitle = {Anti-{Oversmoothing} in {Deep} {Vision} {Transformers} via the {Fourier} {Domain} {Analysis}},
	doi = {10.48550/arXiv.2203.05962},
	abstract = {Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1\% performance gains "for free" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Wang, Peihao and Zheng, Wenqing and Chen, Tianlong and Wang, Zhangyang},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05962 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: International Conference on Learning Representations (ICLR), 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\4BQ3F2QQ\\Wang et al. - 2022 - Anti-Oversmoothing in Deep Vision Transformers via.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\T9BTY6PV\\2203.html:text/html},
}

@article{zhang_neural_2024,
	title = {Neural {Prompt} {Search}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2024.3435939},
	abstract = {The size of vision models has grown exponentially over the last few years, especially after the emergence of Vision Transformer. This has motivated the development of parameter-efficient tuning methods, such as learning adapter layers or visual prompt tokens, which allow a tiny portion of model parameters to be trained whereas the vast majority obtained from pre-training are frozen. However, designing a proper tuning method is non-trivial: one might need to try out a lengthy list of design choices, not to mention that each downstream dataset often requires custom designs. In this paper, we view the existing parameter-efficient tuning methods as “prompt modules” and propose Neural prOmpt seArcH (NOAH), a novel approach that learns, for large vision models, the optimal design of prompt modules through a neural architecture search algorithm, specifically for each downstream dataset. By conducting extensive experiments on over 20 vision datasets, we demonstrate that NOAH (i) is superior to individual prompt modules, (ii) has good few-shot learning ability, and (iii) is domain-generalizable. The code and models are available at https://github.com/ZhangYuanhan-AI/NOAH.},
	urldate = {2024-08-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Task analysis, Computer architecture, Tuning, Computer vision, Transfer learning, Adaptation models, Transformers, Parameter-efficient Tuning, Transfer Learning},
	pages = {1--14},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\WULW3AFC\\10614881.html:text/html},
}

@inproceedings{pfeiffer_adapterfusion_2021,
	address = {Online},
	title = {{AdapterFusion}: {Non}-{Destructive} {Task} {Composition} for {Transfer} {Learning}},
	shorttitle = {{AdapterFusion}},
	doi = {10.18653/v1/2021.eacl-main.39},
	abstract = {Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.},
	urldate = {2024-08-02},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Pfeiffer, Jonas and Kamath, Aishwarya and Rücklé, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	pages = {487--503},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\BKJQAJ3H\\Pfeiffer et al. - 2021 - AdapterFusion Non-Destructive Task Composition fo.pdf:application/pdf},
}

@article{wang_fine-grained_2023,
	title = {Fine-{Grained} {Retrieval} {Prompt} {Tuning}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	doi = {10.1609/aaai.v37i2.25363},
	abstract = {Fine-grained object retrieval aims to learn discriminative representation to retrieve visually similar objects. However, existing top-performing works usually impose pairwise similarities on the semantic embedding spaces or design a localization sub-network to continually fine-tune the entire model in limited data scenarios, thus resulting in convergence to suboptimal solutions. In this paper, we develop Fine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained model to perform the fine-grained retrieval task from the perspectives of sample prompting and feature adaptation. Specifically, FRPT only needs to learn fewer parameters in the prompt and adaptation instead of fine-tuning the entire model, thus solving the issue of convergence to suboptimal solutions caused by fine-tuning the entire model. Technically, a discriminative perturbation prompt (DPP) is introduced and deemed as a sample prompting process, which amplifies and even exaggerates some discriminative elements contributing to category prediction via a content-aware inhomogeneous sampling operation. In this way, DPP can make the fine-grained retrieval task aided by the perturbation prompts close to the solved task during the original pre-training. Thereby, it preserves the generalization and discrimination of representation extracted from input samples. Besides, a category-specific awareness head is proposed and regarded as feature adaptation, which removes the species discrepancies in features extracted by the pre-trained model using category-guided instance normalization. And thus, it makes the optimized features only include the discrepancies among subcategories. Extensive experiments demonstrate that our FRPT with fewer learnable parameters achieves the state-of-the-art performance on three widely-used fine-grained datasets.},
	language = {en},
	number = {2},
	urldate = {2024-08-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Shijie and Chang, Jianlong and Wang, Zhihui and Li, Haojie and Ouyang, Wanli and Tian, Qi},
	month = jun,
	year = {2023},
	note = {Number: 2},
	keywords = {CV: Image and Video Retrieval},
	pages = {2644--2652},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\V4TT42VS\\Wang et al. - 2023 - Fine-Grained Retrieval Prompt Tuning.pdf:application/pdf},
}

@article{wang_aa-trans_2023,
	title = {{AA}-trans: {Core} attention aggregating transformer with information entropy selector for fine-grained visual classification},
	volume = {140},
	issn = {0031-3203},
	shorttitle = {{AA}-trans},
	doi = {10.1016/j.patcog.2023.109547},
	abstract = {The task of fine-grained visual classification (FGVC) is to distinguish targets from subordinate classifications. Since fine-grained images have the inherent characteristic of large inter-class variances and small intra-class variances, it is considered an extremely difficult task. Most existing approaches adopt CNN-based networks as feature extractors, which causes the extracted discriminative regions to contain most parts of the object in this way, thus failing to locate the really important parts. Recently, the vision transformer (ViT) has demonstrated its power on a wide range of image tasks, which uses an attention mechanism to capture global contextual information to establish a remote dependency on the target and thus extract more powerful features. Nevertheless, the ViT model still focuses more on global coarse-grained information rather than local fine-grained information, which may lead to its undesirable performance in fine-grained image classification. To this end, we redesigned an attention aggregating transformer (AA-Trans) to better capture minor differences among images by improving the ViT structure in this paper. In detail, we propose a core attention aggregator (CAA), which enables better information sharing between each transformer layer. Besides, we further propose an innovative information entropy selector (IES) to guide the network in acquiring discriminative parts of the image precisely. Extensive experiments show that our proposed model structure can achieve a new state-of-the-art performance on several mainstream datasets.},
	urldate = {2024-08-02},
	journal = {Pattern Recognition},
	author = {Wang, Qi and Wang, JianJun and Deng, Hongyu and Wu, Xue and Wang, Yazhou and Hao, Gefei},
	month = aug,
	year = {2023},
	keywords = {Image classification, Vision transformer, Attention aggregator, Fine-grained visual, Information entropy},
	pages = {109547},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\3ELKS6XV\\S0031320323002479.html:text/html},
}

@inproceedings{wang_pyramid_2021,
	title = {Pyramid {Vision} {Transformer}: {A} {Versatile} {Backbone} for {Dense} {Prediction} without {Convolutions}},
	shorttitle = {Pyramid {Vision} {Transformer}},
	doi = {10.1109/ICCV48922.2021.00061},
	abstract = {Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.},
	urldate = {2024-08-02},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Computer vision, Feature extraction, Image resolution, Semantics, Object detection, Transformers, Recognition and classification, Costs, Detection and localization in 2D and 3D, grouping and shape, Segmentation},
	pages = {548--558},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\6Q39R9SZ\\9711179.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\CCBSQBKA\\Wang et al. - 2021 - Pyramid Vision Transformer A Versatile Backbone f.pdf:application/pdf},
}

@inproceedings{liu_swin_2021,
	address = {Montreal, QC, Canada},
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-812-5},
	shorttitle = {Swin {Transformer}},
	doi = {10.1109/ICCV48922.2021.00986},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classiﬁcation (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneﬁcial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = oct,
	year = {2021},
	pages = {9992--10002},
	file = {Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:C\:\\Users\\edwin\\Zotero\\storage\\2BMFGL5H\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}

@misc{zhang_adding_2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({\textless}50k) and large ({\textgreater}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	month = nov,
	year = {2023},
	note = {arXiv:2302.05543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Multimedia, Computer Science - Human-Computer Interaction, Computer Science - Graphics},
	annote = {Comment: Codes and Supplementary Material: https://github.com/lllyasviel/ControlNet},
}

@article{larese_multiscale_2014,
	title = {Multiscale recognition of legume varieties based on leaf venation images},
	volume = {41},
	issn = {0957-4174},
	doi = {10.1016/j.eswa.2014.01.029},
	abstract = {In this work we propose an automatic low cost procedure aimed at classifying legume species and varieties based exclusively on the characterization and analysis of the leaf venation network. The identification of leaf venation patterns which are characteristic for each species or variety is not an easy task since in some situations (specially for cultivars from the same species) the vein differences are visually indistinguishable for humans. The proposed procedure takes as input leaf images acquired using a standard scanner, processes the images in order to segment the veins at different scales, and measures different traits on them. We use these features in combination with modern automatic classifiers and feature selection techniques in order to perform recognition. The process was initially applied to recognize three different legumes in order to evaluate the improvements over previous works in the literature, and then it was employed to distinguish three diverse soybean cultivars. The results show the improvements achieved by the usage of the multiscale features. The cultivar recognition is a more challenging problem, since the experts cannot distinguish evident differences in plain sight. However, we achieve acceptable classification results. We also analyze the feature relevance and identify, for each classifier, a small set of distinctive traits to differentiate the species and varieties.},
	number = {10},
	urldate = {2024-08-02},
	journal = {Expert Systems with Applications},
	author = {Larese, Mónica G. and Bayá, Ariel E. and Craviotto, Roque M. and Arango, Miriam R. and Gallo, Carina and Granitto, Pablo M.},
	month = aug,
	year = {2014},
	keywords = {Image classification, Image analysis, Cultivars recognition, Multiscale vein images},
	pages = {4638--4647},
	file = {Full Text:C\:\\Users\\edwin\\Zotero\\storage\\GADCNK29\\Larese et al. - 2014 - Multiscale recognition of legume varieties based o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\49D3AFAW\\S0957417414000529.html:text/html},
}

@article{mohanty_using_2016,
	title = {Using {Deep} {Learning} for {Image}-{Based} {Plant} {Disease} {Detection}},
	volume = {7},
	issn = {1664-462X},
	doi = {10.3389/fpls.2016.01419},
	abstract = {{\textless}p{\textgreater}Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-08-02},
	journal = {Frontiers in Plant Science},
	author = {Mohanty, Sharada P. and Hughes, David P. and Salathé, Marcel},
	month = sep,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {machine learning, deep learning, Crop diseases, digital epidemiology, Disease diagnosis},
	file = {Full Text:C\:\\Users\\edwin\\Zotero\\storage\\5XAW7BJC\\Mohanty et al. - 2016 - Using Deep Learning for Image-Based Plant Disease .pdf:application/pdf},
}

@article{yu_spare_2022,
	title = {{SPARE}: {Self}-supervised part erasing for ultra-fine-grained visual categorization},
	volume = {128},
	issn = {0031-3203},
	shorttitle = {{SPARE}},
	doi = {10.1016/j.patcog.2022.108691},
	abstract = {This paper presents SPARE, a self-supervised part erasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e., understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.},
	urldate = {2024-08-02},
	journal = {Pattern Recognition},
	author = {Yu, Xiaohan and Zhao, Yang and Gao, Yongsheng},
	month = aug,
	year = {2022},
	keywords = {Fine-grained visual categorization, Ultra-fine-grained visual categorization, Random part erasing, Self-Supervised part erasing, Weakly-supervised part segmentation},
	pages = {108691},
}

@misc{park_fine-grained_2023,
	title = {Fine-{Grained} {Self}-{Supervised} {Learning} with {Jigsaw} {Puzzles} for {Medical} {Image} {Classification}},
	doi = {10.48550/arXiv.2308.05770},
	abstract = {Classifying fine-grained lesions is challenging due to minor and subtle differences in medical images. This is because learning features of fine-grained lesions with highly minor differences is very difficult in training deep neural networks. Therefore, in this paper, we introduce Fine-Grained Self-Supervised Learning(FG-SSL) method for classifying subtle lesions in medical images. The proposed method progressively learns the model through hierarchical block such that the cross-correlation between the fine-grained Jigsaw puzzle and regularized original images is close to the identity matrix. We also apply hierarchical block for progressive fine-grained learning, which extracts different information in each step, to supervised learning for discovering subtle differences. Our method does not require an asymmetric model, nor does a negative sampling strategy, and is not sensitive to batch size. We evaluate the proposed fine-grained self-supervised learning method on comprehensive experiments using various medical image recognition datasets. In our experiments, the proposed method performs favorably compared to existing state-of-the-art approaches on the widely-used ISIC2018, APTOS2019, and ISIC2017 datasets.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Park, Wongi and Ryu, Jongbin},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05770 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\KMRKUTTH\\Park and Ryu - 2023 - Fine-Grained Self-Supervised Learning with Jigsaw .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\VRTEDNQ2\\2308.html:text/html},
}

@inproceedings{lehr_automated_2020,
	title = {Automated {Optical} {Inspection} {Using} {Anomaly} {Detection} and {Unsupervised} {Defect} {Clustering}},
	volume = {1},
	doi = {10.1109/ETFA46521.2020.9212172},
	abstract = {Neural networks have proven to be extraordinarily successful in many computer vision applications. But the approaches used to train neural networks require large datasets of annotated images, which requires a solid amount of human time to prepare those datasets. To facilitate the adoption of machine learning based technologies in industrial computer vision applications, this paper presents a two-step unsupervised learning approach for anomaly detection with further defect clusterization. In the first stage, the defects are not explicitly learned, but are interpreted as an anomaly or novelty based on the dataset of defect-free samples. In a second stage, the anomalies detected in the first stage are clustered in unsupervised manner and classified into meaningful categories by experts with process knowledge (e.g. critical or non-critical defect). This paper presents a first small dataset containing one industrial object with a complex shape. The object is made of aluminium and is shown both free of defects and defective. Based on this, recommendations are given for an acquisition setup for a large, extensive dataset. Most of the existing papers are studying the approaches for uniform surface (texture) inspection. The specifics of this research is to identify defects on rigid bodies, which exhibit highly non uniform texture in the image. State of the art methods were evaluated and improved to increase the classification accuracy. With a fine-tuned ResNet-18 it was possible to achieve 100\% accuracy for defective and defect-free images.},
	urldate = {2024-08-02},
	booktitle = {2020 25th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Lehr, Jan and Sargsyan, Alik and Pape, Martin and Philipps, Jan and Krüger, Jörg},
	month = sep,
	year = {2020},
	note = {ISSN: 1946-0759},
	keywords = {Neural networks, Computer vision, Deep Learning, Shape, Convolutional Neural Networks, Anomaly Detection, Convolutional Auto Encoder, Defect Clustering, Novalty Detection, Optical fiber networks, Optical imaging, Optical Inspection, Solids, Surface texture},
	pages = {1235--1238},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\QU35KWYP\\9212172.html:text/html},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\BPWVP8P8\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\edwin\\Zotero\\storage\\JCPPSXUA\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}

@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	doi = {10.48550/arXiv.2304.12210},
	abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	month = jun,
	year = {2023},
	note = {arXiv:2304.12210 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\IDURWZWN\\Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\LHW98ITB\\2304.html:text/html},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2024-08-02},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\FRGUEYPN\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\Y2Y9HDQ4\\1503.html:text/html},
}

@article{zhang_self-distillation_2022,
	title = {Self-{Distillation}: {Towards} {Efficient} and {Compact} {Neural} {Networks}},
	volume = {44},
	issn = {0162-8828},
	shorttitle = {Self-{Distillation}},
	doi = {10.1109/TPAMI.2021.3067100},
	abstract = {Remarkable achievements have been obtained by deep neural networks in the last several years. However, the breakthrough in neural networks accuracy is always accompanied by explosive growth of computation and parameters, which leads to a severe limitation of model deployment. In this paper, we propose a novel knowledge distillation technique named self-distillation to address this problem. Self-distillation attaches several attention modules and shallow classifiers at different depths of neural networks and distills knowledge from the deepest classifier to the shallower classifiers. Different from the conventional knowledge distillation methods where the knowledge of the teacher model is transferred to another student model, self-distillation can be considered as knowledge transfer in the same model - from the deeper layers to the shallow layers. Moreover, the additional classifiers in self-distillation allow the neural network to work in a dynamic manner, which leads to a much higher acceleration. Experiments demonstrate that self-distillation has consistent and significant effectiveness on various neural networks and datasets. On average, 3.49 and 2.32 percent accuracy boost are observed on CIFAR100 and ImageNet. Besides, experiments show that self-distillation can be combined with other model compression methods, including knowledge distillation, pruning and lightweight model design.},
	language = {English},
	number = {08},
	urldate = {2024-08-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Linfeng and Bao, Chenglong and Ma, Kaisheng},
	month = aug,
	year = {2022},
	note = {Publisher: IEEE Computer Society},
	pages = {4388--4403},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\W6WT9YCN\\Zhang et al. - 2022 - Self-Distillation Towards Efficient and Compact N.pdf:application/pdf},
}

@inproceedings{lester_power_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	doi = {10.18653/v1/2021.emnlp-main.243},
	abstract = {In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.},
	urldate = {2024-08-02},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {3045--3059},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\T42INGUX\\Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf:application/pdf},
}

@inproceedings{kong_low-rank_2017,
	address = {Honolulu, HI},
	title = {Low-{Rank} {Bilinear} {Pooling} for {Fine}-{Grained} {Classification}},
	isbn = {978-1-5386-0457-1},
	doi = {10.1109/CVPR.2017.743},
	abstract = {Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of ﬁnegrained classiﬁcation tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a lowrank bilinear classiﬁer. The resulting classiﬁer can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kong, Shu and Fowlkes, Charless},
	month = jul,
	year = {2017},
	pages = {7025--7034},
	file = {Kong and Fowlkes - 2017 - Low-Rank Bilinear Pooling for Fine-Grained Classif.pdf:C\:\\Users\\edwin\\Zotero\\storage\\UP334NDU\\Kong and Fowlkes - 2017 - Low-Rank Bilinear Pooling for Fine-Grained Classif.pdf:application/pdf},
}

@inproceedings{li_is_2017,
	address = {Venice},
	title = {Is {Second}-{Order} {Information} {Helpful} for {Large}-{Scale} {Visual} {Recognition}?},
	isbn = {978-1-5386-1032-9},
	doi = {10.1109/ICCV.2017.228},
	abstract = {By stacking layers of convolution and nonlinearity, convolutional networks (ConvNets) effectively learn from lowlevel to high-level features and discriminative representations. Since the end goal of large-scale recognition is to delineate complex boundaries of thousands of classes, adequate exploration of feature distributions is important for realizing full potentials of ConvNets. However, state-of-theart works concentrate only on deeper or wider architecture design, while rarely exploring feature statistics higher than ﬁrst-order. We take a step towards addressing this problem. Our method consists in covariance pooling, instead of the most commonly used ﬁrst-order pooling, of highlevel convolutional features. The main challenges involved are robust covariance estimation given a small sample of large-dimensional features and usage of the manifold structure of covariance matrices. To address these challenges, we present a Matrix Power Normalized Covariance (MPNCOV) method. We develop forward and backward propagation formulas regarding the nonlinear matrix functions such that MPN-COV can be trained end-to-end. In addition, we analyze both qualitatively and quantitatively its advantage over the well-known Log-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV we achieve over 4\%, 3\% and 2.5\% gains for AlexNet, VGG-M and VGG-16, respectively; integration of MPN-COV into 50-layer ResNet outperforms ResNet-101 and is comparable to ResNet-152. The source code will be available on the project page: http://www.peihuali.org/MPN-COV.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Li, Peihua and Xie, Jiangtao and Wang, Qilong and Zuo, Wangmeng},
	month = oct,
	year = {2017},
	pages = {2089--2097},
	file = {Li et al. - 2017 - Is Second-Order Information Helpful for Large-Scal.pdf:C\:\\Users\\edwin\\Zotero\\storage\\F23X7AJ8\\Li et al. - 2017 - Is Second-Order Information Helpful for Large-Scal.pdf:application/pdf},
}

@inproceedings{li_towards_2018,
	address = {Salt Lake City, UT},
	title = {Towards {Faster} {Training} of {Global} {Covariance} {Pooling} {Networks} by {Iterative} {Matrix} {Square} {Root} {Normalization}},
	isbn = {978-1-5386-6420-9},
	doi = {10.1109/CVPR.2018.00105},
	abstract = {Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical ﬁrst-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefﬁcient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-toend training of global covariance pooling networks. At the core of our method is a meta-layer designed with loopembedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By ﬁnetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging ﬁnegrained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV.},
	language = {en},
	urldate = {2024-08-02},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Li, Peihua and Xie, Jiangtao and Wang, Qilong and Gao, Zilin},
	month = jun,
	year = {2018},
	pages = {947--955},
	file = {Li et al. - 2018 - Towards Faster Training of Global Covariance Pooli.pdf:C\:\\Users\\edwin\\Zotero\\storage\\4KVL5HNT\\Li et al. - 2018 - Towards Faster Training of Global Covariance Pooli.pdf:application/pdf},
}

@inproceedings{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2024-08-03},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3519--3529},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\BPRU6P5V\\Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf:application/pdf;Supplementary PDF:C\:\\Users\\edwin\\Zotero\\storage\\XZ76MMCR\\Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf:application/pdf},
}

@inproceedings{rios_down-sampling_2024,
	title = {Down-{Sampling} {Inter}-{Layer} {Adapter} for {Parameter} and {Computation} {Efficient} {Ultra}-{Fine}-{Grained} {Image} {Recognition}},
	copyright = {All rights reserved},
	doi = {10.48550/arXiv.2409.11051},
	abstract = {Ultra-fine-grained image recognition (UFGIR) categorizes objects with extremely small differences between classes, such as distinguishing between cultivars within the same species, as opposed to species-level classification in fine-grained image recognition (FGIR). The difficulty of this task is exacerbated due to the scarcity of samples per category. To tackle these challenges we introduce a novel approach employing down-sampling inter-layer adapters in a parameter-efficient setting, where the backbone parameters are frozen and we only fine-tune a small set of additional modules. By integrating dual-branch down-sampling, we significantly reduce the number of parameters and floating-point operations (FLOPs) required, making our method highly efficient. Comprehensive experiments on ten datasets demonstrate that our approach obtains outstanding accuracy-cost performance, highlighting its potential for practical applications in resource-constrained environments. In particular, our method increases the average accuracy by at least 6.8{\textbackslash}\% compared to other methods in the parameter-efficient setting while requiring at least 123x less trainable parameters compared to current state-of-the-art UFGIR methods and reducing the FLOPs by 30{\textbackslash}\% in average compared to other methods.},
	urldate = {2024-09-18},
	booktitle = {Computer {Vision} – {ECCV} 2024 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Rios, Edwin Arkel and Oyerinde, Femiloye and Hu, Min-Chun and Lai, Bo-Cheng},
	month = sep,
	year = {2024},
	note = {arXiv:2409.11051 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2, I.4},
	annote = {Comment: Accepted to ECCV 2024 Workshop on Efficient Deep Learning for Foundation Models (EFM). Main: 13 pages, 3 figures, 2 tables. Appendix: 3 pages, 1 table. Total: 16 pages, 3 figures, 4 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\edwin\\Zotero\\storage\\FKL9RTE9\\Rios et al. - 2024 - Down-Sampling Inter-Layer Adapter for Parameter an.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\UKSH6EMI\\2409.html:text/html},
}

@inproceedings{yun_cutmix_2019,
	address = {Seoul, Korea (South)},
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72814-803-8},
	shorttitle = {{CutMix}},
	doi = {10.1109/ICCV.2019.00612},
	abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classiﬁers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefﬁciency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efﬁcient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classiﬁcation tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classiﬁer, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
	language = {en},
	urldate = {2024-11-15},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Oh, Seong Joon and Yoo, Youngjoon and Choe, Junsuk},
	month = oct,
	year = {2019},
	pages = {6022--6031},
	file = {Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:C\:\\Users\\edwin\\Zotero\\storage\\3L8W6SN6\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf:application/pdf},
}

@article{huang_snapmix_2021,
	title = {{SnapMix}: {Semantically} {Proportional} {Mixing} for {Augmenting} {Fine}-grained {Data}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{SnapMix}},
	doi = {10.1609/aaai.v35i2.16255},
	abstract = {Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly based on the mixture proportion of image pixels. As the main discriminative information of a ﬁne-grained image usually resides in subtle regions, methods along this line are prone to heavy label noise in ﬁne-grained recognition. We propose in this paper a novel scheme, termed as Semantically Proportional Mixing (SnapMix), which exploits class activation map (CAM) to lessen the label noise in augmenting ﬁne-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition, and allows for asymmetric mixing operations and ensures semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches on various datasets and under different network depths. Furthermore, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a solid baseline for ﬁne-grained recognition.},
	language = {en},
	number = {2},
	urldate = {2024-11-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Shaoli and Wang, Xinchao and Tao, Dacheng},
	month = may,
	year = {2021},
	pages = {1628--1636},
	file = {Huang et al. - 2021 - SnapMix Semantically Proportional Mixing for Augm.pdf:C\:\\Users\\edwin\\Zotero\\storage\\8RD5YWFN\\Huang et al. - 2021 - SnapMix Semantically Proportional Mixing for Augm.pdf:application/pdf},
}

@inproceedings{cao_training_2022,
	address = {Cham},
	title = {Training {Vision} {Transformers} with only 2040 {Images}},
	isbn = {978-3-031-19806-9},
	doi = {10.1007/978-3-031-19806-9_13},
	abstract = {Vision Transformers (ViTs) is emerging as an alternative to convolutional neural networks (CNNs) for visual recognition. They achieve competitive results with CNNs but the lack of the typical convolutional inductive bias makes them more data-hungry than common CNNs. They are often pretrained on JFT-300M or at least ImageNet and few works study training ViTs with limited data. In this paper, we investigate how to train ViTs with limited data (e.g., 2040 images). We give theoretical analyses that our method (based on parametric instance discrimination) is superior to other methods in that it can capture both feature alignment and instance similarities. We achieve state-of-the-art results when training from scratch on 7 small datasets under various ViT backbones. We also investigate the transferring ability of small datasets and find that representations learned from small datasets can even improve large-scale ImageNet training.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Cao, Yun-Hao and Yu, Hao and Wu, Jianxin},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Small data, Train from scratch, Vision transformers},
	pages = {220--237},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\S23SLYEH\\Cao et al. - 2022 - Training Vision Transformers with only 2040 Images.pdf:application/pdf},
}

@inproceedings{bae_self-contrastive_2023,
	series = {{AAAI}'23/{IAAI}'23/{EAAI}'23},
	title = {Self-contrastive learning: single-viewed supervised contrastive framework using sub-network},
	volume = {37},
	isbn = {978-1-57735-880-0},
	shorttitle = {Self-contrastive learning},
	doi = {10.1609/aaai.v37i1.25091},
	abstract = {Contrastive loss has significantly improved performance in supervised classification tasks by using a multi-viewed framework that leverages augmentation and label information. The augmentation enables contrast with another view of a single image but enlarges training time and memory usage. To exploit the strength of multi-views while avoiding the high computation cost, we introduce a multi-exit architecture that outputs multiple features of a single image in a single-viewed framework. To this end, we propose Self-Contrastive (Self-Con) learning, which self-contrasts within multiple outputs from the different levels of a single network. The multi-exit architecture efficiently replaces multi-augmented images and leverages various information from different layers of a network. We demonstrate that SelfCon learning improves the classification performance of the encoder network, and empirically analyze its advantages in terms of the single-view and the subnetwork. Furthermore, we provide theoretical evidence of the performance increase based on the mutual information bound. For ImageNet classification on ResNet-50, SelfCon improves accuracy by +0.6\% with 59\% memory and 48\% time of Supervised Contrastive learning, and a simple ensemble of multi-exit outputs boosts performance up to +1.5\%.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{Fifth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence} and {Thirteenth} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Bae, Sangmin and Kim, Sungnyun and Ko, Jongwoo and Lee, Gihun and Noh, Seungjong and Yun, Se-Young},
	month = feb,
	year = {2023},
	pages = {197--205},
	file = {Full Text:C\:\\Users\\edwin\\Zotero\\storage\\SW9DILDY\\Bae et al. - 2023 - Self-contrastive learning single-viewed supervise.pdf:application/pdf},
}

@inproceedings{scheibenreif_parameter_2024,
	address = {Seattle, WA, USA},
	title = {Parameter {Efficient} {Self}-{Supervised} {Geospatial} {Domain} {Adaptation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350353006},
	doi = {10.1109/CVPR52733.2024.02630},
	abstract = {As large-scale foundation models become publicly available for different domains, efﬁciently adapting them to individual downstream applications and additional data modalities has turned into a central challenge. For example, foundation models for geospatial and satellite remote sensing applications are commonly trained on large optical RGB or multi-spectral datasets, although data from a wide variety of heterogeneous sensors are available in the remote sensing domain. This leads to signiﬁcant discrepancies between pre-training and downstream target data distributions for many important applications. Fine-tuning large foundation models to bridge that gap incurs high computational cost and can be infeasible when target datasets are small. In this paper, we address the question of how large, pretrained foundational transformer models can be efﬁciently adapted to downstream remote sensing tasks involving different data modalities or limited dataset size. We present a self-supervised adaptation method that boosts downstream linear evaluation accuracy of different foundation models by 4-6\% (absolute) across 8 remote sensing datasets while outperforming full ﬁne-tuning when training only 1-2\% of the model parameters. Our method signiﬁcantly improves label efﬁciency and increases few-shot accuracy by 6-10\% 1 on different datasets .},
	language = {en},
	urldate = {2025-03-01},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Scheibenreif, Linus and Mommert, Michael and Borth, Damian},
	month = jun,
	year = {2024},
	pages = {27841--27851},
	file = {Scheibenreif et al. - 2024 - Parameter Efficient Self-Supervised Geospatial Dom.pdf:C\:\\Users\\edwin\\Zotero\\storage\\8K4EEC9R\\Scheibenreif et al. - 2024 - Parameter Efficient Self-Supervised Geospatial Dom.pdf:application/pdf},
}

@inproceedings{bafghi_parameter_2024,
	title = {Parameter {Efficient} {Fine}-tuning of {Self}-supervised {ViTs} without {Catastrophic} {Forgetting}},
	doi = {10.1109/CVPRW63382.2024.00371},
	abstract = {Artificial neural networks often suffer from catastrophic forgetting, where learning new concepts leads to a complete loss of previously acquired knowledge. We observe that this issue is particularly magnified in vision transformers (ViTs), where post-pre-training and fine-tuning on new tasks can significantly degrade the model’s original general abilities. For instance, a DINO ViT-Base/16 pre-trained on ImageNet-1k loses over 70\% accuracy on ImageNet-1k after just 10 iterations of fine-tuning on CIFAR-100. Overcoming this stability-plasticity dilemma is crucial for enabling ViTs to continuously learn and adapt to new domains while preserving their initial knowledge. In this work, we study two new parameter-efficient fine-tuning strategies: (1) Block Expansion, and (2) Low-rank adaptation (LoRA). Our experiments reveal that using either Block Expansion or LoRA on self-supervised pre-trained ViTs surpass fully fine-tuned ViTs in new domains while offering significantly greater parameter efficiency. Notably, we find that Block Expansion experiences only a minimal performance drop in the pre-training domain, thereby effectively mitigating catastrophic forgetting in pre-trained ViTs1.},
	urldate = {2025-03-01},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Bafghi, Reza Akbarian and Harilal, Nidhin and Monteleoni, Claire and Raissi, Maziar},
	month = jun,
	year = {2024},
	note = {ISSN: 2160-7516},
	keywords = {Computer vision, Artificial neural networks, Knowledge engineering, Conferences, Adaptation models, Transformers, Catastrophic Forgetting, Learning (artificial intelligence), Parameter Efficient Fine-tuning, Self-supervised, ViT},
	pages = {3679--3684},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\6NNJD3PR\\10678325.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\8BWKIP4W\\Bafghi et al. - 2024 - Parameter Efficient Fine-tuning of Self-supervised.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2025-03-01},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Task analysis, Training, Unsupervised learning, Visualization, Dictionaries, Buildings, Loss measurement},
	pages = {9726--9735},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\PK29LEX5\\9157636.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\FBW6Q3FC\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}

@inproceedings{dosovitskiy_discriminative_2014,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Convolutional} {Neural} {Networks}},
	volume = {27},
	abstract = {Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).},
	urldate = {2025-03-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\NYZCXFSU\\Dosovitskiy et al. - 2014 - Discriminative Unsupervised Feature Learning with .pdf:application/pdf},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\edwin\\Zotero\\storage\\UGMX7EDK\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\L5AP44RR\\1807.html:text/html},
}

@inproceedings{khosla_supervised_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Supervised contrastive learning},
	isbn = {978-1-71382-954-6},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsu-pervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the Ima-geNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = dec,
	year = {2020},
	pages = {18661--18673},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\26ZRX264\\Khosla et al. - 2020 - Supervised contrastive learning.pdf:application/pdf},
}

@inproceedings{go_channel_2025,
	title = {Channel {Propagation} {Networks} for {Refreshable} {Vision} {Transformer}},
	language = {en},
	urldate = {2025-03-04},
	author = {Go, Junhyeong and Ryu, Jongbin},
	year = {2025},
	pages = {1353--1362},
	file = {Full Text PDF:C\:\\Users\\edwin\\Zotero\\storage\\FLTLHGFS\\Go and Ryu - 2025 - Channel Propagation Networks for Refreshable Visio.pdf:application/pdf},
}

@article{liu_transformer_2022,
	title = {Transformer with peak suppression and knowledge guidance for fine-grained image recognition},
	volume = {492},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2022.04.037},
	abstract = {Fine-grained image recognition is challenging because discriminative clues are usually fragmented, whether from a single image or multiple images. Despite their significant improvements, the majority of existing methods still focus on the most discriminative parts from a single image, ignoring informative details in other regions and lacking consideration of clues from other associated images. In this paper, we analyze the difficulties of fine-grained image recognition from a new perspective and propose a transformer architecture with the peak suppression module and knowledge guidance module, which respects the diversification of discriminative features in a single image and the aggregation of discriminative clues among multiple images. Specifically, the peak suppression module first utilizes a linear projection to convert the input image into sequential tokens. It then blocks the token based on the attention response generated by the transformer encoder. This module penalizes the attention to the most discriminative parts in the feature learning process, therefore, enhancing the information exploitation of the neglected regions. The knowledge guidance module compares the image-based representation generated from the peak suppression module with the learnable knowledge embedding set to obtain the knowledge response coefficients. Afterwards, it formalizes the knowledge learning as a classification problem using response coefficients as the classification scores. Knowledge embeddings and image-based representations are updated during training simultaneously so that the knowledge embedding includes a large number of discriminative clues for different images of the same category. Finally, we incorporate the acquired knowledge embeddings into the image-based representations as comprehensive representations, leading to significantly higher recognition performance. Extensive evaluations on the six popular datasets demonstrate the advantage of the proposed method in performance. The source code and models will be available online after the acceptance of the paper.},
	urldate = {2025-03-04},
	journal = {Neurocomputing},
	author = {Liu, Xinda and Wang, Lili and Han, Xiaoguang},
	month = jul,
	year = {2022},
	keywords = {Fine-grained image recognition, Food recognition, Knowledge guidance, Peak suppression, Vision transformer},
	pages = {137--149},
	file = {ScienceDirect Snapshot:C\:\\Users\\edwin\\Zotero\\storage\\6UVH6KA8\\S0925231222004052.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\57ECI67N\\Liu et al. - 2022 - Transformer with peak suppression and knowledge gu.pdf:application/pdf},
}

@article{zhang_s3mix_2023,
	title = {{S3Mix}: {Same} {Category} {Same} {Semantics} {Mixing} for {Augmenting} {Fine}-grained {Images}},
	volume = {20},
	issn = {1551-6857},
	shorttitle = {{S3Mix}},
	doi = {10.1145/3605892},
	abstract = {Data augmentation is a common technique to improve the generalization performance of models for image classification. Although methods such as Mixup and CutMix that mix images randomly are indeed instrumental in general image classification, randomly swapping or masking regions is not friendly to fine-grained images, since the key to fine-grained image classification precisely lies in discriminative and informative regions, and it is unreasonable to generate labels solely consistent with the proportion of synthesis. Some erasing methods like Cutout even endanger fine-grained image classification because of erasing the discriminative regions by chance. In this article, we propose the Same Category Same Semantics Mixing method (S3Mix) corresponding to the characteristics of fine-grained images. Specifically, we limit the mixture to regions of the same category and semantics. The core of the method is two constraints. The exchange with the semantic region ensures the discrimination and semantics integrity of the generated image, and the exchange in the same class avoids the problem of unreasonable label generation. At the same time, we propose a homology loss to promote the semantic relationship between the generated positive image pairs. Experiments have been conducted on four fine-grained datasets, and the results show the proposed method is superior to the traditional image augmentation methods as well as some fine-grained data augmentation methods.},
	number = {1},
	urldate = {2025-03-04},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Zhang, Zi-Chao and Chen, Zhen-Duo and Xie, Zhen-Yu and Luo, Xin and Xu, Xin-Shun},
	month = aug,
	year = {2023},
	pages = {9:1--9:16},
}

@inproceedings{zhang_intra-class_2021,
	title = {Intra-class {Part} {Swapping} for {Fine}-{Grained} {Image} {Classification}},
	doi = {10.1109/WACV48630.2021.00325},
	abstract = {Recent works such as Mixup and CutMix have demonstrated the effectiveness of augmenting training data for deep models. These methods generate new data by generally blending random image contents and mixing their labels proportionally. However, this strategy tends to produce unreasonable training samples for fine-grained recognition, leading to limited improvement. This is because mixing random image contents may potentially produce images containing destructed object structures. Further, as the category differences mainly reside in small part regions, mixing labels proportionally to the number of mixed pixels might result in label noisy problem. To augment more reasonable training data, we propose Intra-class Part Swapping (InPS) that produces new data by performing attention-guided content swapping on input pairs from the same class. Compared with previous approaches, InPS avoids introducing noisy labels and ensures a likely holistic structure of objects in generated images. We demonstrate InPS outperforms the most recent augmentation approaches in both fine-grained recognition and weakly object localization. Further, by simply incorporating the mid-level feature learning, our proposed method achieves state-of-the-art performance in the literature while maintaining the simplicity and inference efficiency. Our code is publicly available†.},
	urldate = {2025-03-04},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Zhang, Lianbo and Huang, Shaoli and Liu, Wei},
	month = jan,
	year = {2021},
	note = {ISSN: 2642-9381},
	keywords = {Conferences, Data models, Image recognition, Location awareness, Noise measurement, Training, Training data},
	pages = {3208--3217},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\edwin\\Zotero\\storage\\GTBHCWKS\\9423080.html:text/html;Submitted Version:C\:\\Users\\edwin\\Zotero\\storage\\VIWPYHCK\\Zhang et al. - 2021 - Intra-class Part Swapping for Fine-Grained Image C.pdf:application/pdf},
}
